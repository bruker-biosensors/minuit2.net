using AwesomeAssertions;
using minuit2.net;
using minuit2.net.CostFunctions;
using minuit2.net.Minimizers;
using minuit2.UnitTests.MinimizationProblems;
using minuit2.UnitTests.TestUtilities;
using static minuit2.net.MinimizationExitCondition;

namespace minuit2.UnitTests;

[TestFixture]
[Description("Absolute expectations are generated by iminuit, the Minuit2 wrapper for Python, for the same scenarios.")]
public class The_migrad_minimizer() : Any_parameter_uncertainty_resolving_minimizer(MigradMinimizer)
{
    private static readonly IMinimizer MigradMinimizer = Minimizer.Migrad;
    
    private static IMinimizationResult MinimizeAndRefineErrors(
        ICostFunction cost,
        ParameterConfiguration[] parameterConfigurations, 
        MinimizerConfiguration? minimizerConfiguration = null)
    {
        var result = MigradMinimizer.Minimize(cost, parameterConfigurations, minimizerConfiguration);
        var adjustedCost = cost.WithErrorDefinitionAdjustedWhereRequiredBasedOn(result);
        return HesseErrorCalculator.Refine(result, adjustedCost);
    }
    
    public class When_minimizing_a_least_squares_cost_function
    {
        private readonly ConfigurableLeastSquaresProblem _problem = new CubicPolynomialLeastSquaresProblem();

        [TestCase(false, 100)]
        [TestCase(true, 78)]
        public void yields_the_expected_result(bool hasGradient, int expectedFunctionCalls)
        {
            var cost = _problem.Cost.WithGradient(hasGradient).Build();
            var parameterConfigurations = _problem.ParameterConfigurations.Build();
            
            var result = MigradMinimizer.Minimize(cost, parameterConfigurations);

            result.ShouldFulfill(x =>
            {
                x.ExitCondition.Should().Be(Converged);
                x.IsValid.Should().BeTrue();
                x.NumberOfVariables.Should().Be(4);
                x.NumberOfFunctionCalls.Should().BeCloseTo(expectedFunctionCalls);
                x.CostValue.Should().BeApproximately(12.49);
                x.Parameters.Should().Equal("c0", "c1", "c2", "c3");
                x.ParameterValues.Should().BeApproximately([9.974, -1.959, 0.9898, -0.09931]);
                x.ParameterCovarianceMatrix.Should().BeApproximately(new[,]
                {
                    { 0.005623, -0.004301, 0.000881, -5.271e-05 },
                    { -0.004301, 0.004923, -0.001177, 7.655e-05 },
                    { 0.000881, -0.001177, 0.0003037, -2.067e-05 },
                    { -5.271e-05, 7.655e-05, -2.067e-05, 1.45e-06 }
                });
            });
        }

        [TestCase(false, 31)]
        [TestCase(true, 31)]
        public void with_fixed_parameters_yields_the_expected_result(bool hasGradient, int expectedFunctionCalls)
        {
            var cost = _problem.Cost.WithGradient(hasGradient).Build();
            var parameterConfigurations = _problem.ParameterConfigurations
                .WithParameter(1).Fixed().And
                .WithParameter(3).Fixed().Build();

            var result = MigradMinimizer.Minimize(cost, parameterConfigurations);
            
            result.ShouldFulfill(x =>
            {
                x.ExitCondition.Should().Be(Converged);
                x.IsValid.Should().BeTrue();
                x.NumberOfVariables.Should().Be(2);
                x.NumberOfFunctionCalls.Should().BeCloseTo(expectedFunctionCalls);
                x.CostValue.Should().BeApproximately(437.7);
                x.Parameters.Should().Equal("c0", "c1", "c2", "c3");
                x.ParameterValues.Should().BeApproximately([9.411, -1.97, 1.088, -0.11]);
                x.ParameterCovarianceMatrix.Should().BeApproximately(new[,]
                {
                    { 0.001092, 0.0, -1.918e-05, 0.0 },
                    { 0.0, 0.0, 0.0, 0.0 },
                    { -1.918e-05, 0.0, 6.211e-07, 0.0 },
                    { 0.0, 0.0, 0.0, 0.0 }
                });
            });
        }
        
        [Test]
        public void with_limited_parameters_yields_the_expected_result()
        {
            var cost = _problem.Cost.Build();
            var parameterConfigurations = _problem.ParameterConfigurations
                .WithParameter(0).WithLimits(10.5, null).And
                .WithParameter(3).WithLimits(null, -0.105).Build();
            
            var result = MigradMinimizer.Minimize(cost, parameterConfigurations);
            
            result.ShouldFulfill(x =>
            {
                x.ExitCondition.Should().Be(Converged);
                x.IsValid.Should().BeTrue();
                x.NumberOfVariables.Should().Be(4);
                x.NumberOfFunctionCalls.Should().BeCloseTo(444);
                x.CostValue.Should().BeApproximately(62.34);
                x.Parameters.Should().Equal("c0", "c1", "c2", "c3");
                x.ParameterValues.Should().BeApproximately([10.5, -2.39, 1.082, -0.105]);
                x.ParameterCovarianceMatrix.Should().BeApproximately(new[,]
                {
                    { 7.023e-09, -3.654e-09, 3.124e-10, -1.261e-14 },
                    { -3.654e-09, 0.0002602, -3.344e-05, 5.468e-09 },
                    { 3.124e-10, -3.344e-05, 4.594e-06, -1.873e-09 },
                    { -1.261e-14, 5.468e-09, -1.873e-09, 1.211e-10 }
                }, relativeTolerance: 0.003);
            });
        }
        
        [Test]
        public void with_an_analytical_gradient_and_with_limited_parameters_yields_the_expected_result()
        {
            var cost = _problem.Cost.WithGradient().Build();
            var parameterConfigurations = _problem.ParameterConfigurations
                .WithParameter(0).WithLimits(10.5, null).And
                .WithParameter(3).WithLimits(null, -0.105).Build();

            var result = MigradMinimizer.Minimize(cost, parameterConfigurations);

            // Covariances are slightly different from the numerical-gradient case (above).
            // This is because, in contrast to proper convergence, in case of early termination at parameter limits the
            // assumption of local quadratic behavior (parabolic approximation) around the terminal cost value is
            // generally not fulfilled. Therefore, covariances are not fully robust.
            result.ShouldFulfill(x =>
            {
                x.ExitCondition.Should().Be(Converged);
                x.IsValid.Should().BeTrue();
                x.NumberOfVariables.Should().Be(4);
                x.NumberOfFunctionCalls.Should().BeCloseTo(156);
                x.CostValue.Should().BeApproximately(62.34);
                x.Parameters.Should().Equal("c0", "c1", "c2", "c3");
                x.ParameterValues.Should().BeApproximately([10.5, -2.39, 1.082, -0.105]);
                x.ParameterCovarianceMatrix.Should().BeApproximately(new[,]
                {
                    { 5.482e-09, -2.933e-09, 2.507e-10, -7.976e-15 },
                    { -2.933e-09, 0.0002602, -3.343e-05, 4.309e-09 },
                    { 2.507e-10, -3.343e-05, 4.589e-06, -1.476e-09 },
                    { -7.976e-15, 4.309e-09, -1.476e-09, 9.18e-11 }
                });
            });
        }
        
        [Test]
        public void with_unknown_data_errors_yields_the_expected_result([Values] bool hasGradient)
        {
            var cost = _problem.Cost.WithUnknownYErrors().WithGradient(hasGradient).Build();
            var parameterConfigurations = _problem.ParameterConfigurations.Build();
            
            var result = MigradMinimizer.Minimize(cost, parameterConfigurations);
            
            result.ShouldFulfill(x =>
            {
                x.ExitCondition.Should().Be(Converged);
                x.IsValid.Should().BeTrue();
                x.NumberOfVariables.Should().Be(4);
                x.CostValue.Should().BeApproximately(0.1249);
                x.Parameters.Should().Equal("c0", "c1", "c2", "c3");
                x.ParameterValues.Should().BeApproximately([9.974, -1.959, 0.9898, -0.09931]); 
                x.ParameterCovarianceMatrix.Should().BeApproximately(new[,]
                {
                    { 0.5623, -0.4301, 0.08809, -0.00527 },
                    { -0.4301, 0.4922, -0.1177, 0.007654 },
                    { 0.08809, -0.1177, 0.03036, -0.002067 },
                    { -0.00527, 0.007654, -0.002067, 0.000145 }
                });
            });
        }
        
        [Test]
        public void with_unknown_data_errors_and_subsequently_applying_error_refinement_yields_the_expected_result(
            [Values] bool hasGradient)
        {
            var cost = _problem.Cost.WithUnknownYErrors().WithGradient(hasGradient).Build();
            var parameterConfigurations = _problem.ParameterConfigurations.Build();
            
            var result = MinimizeAndRefineErrors(cost, parameterConfigurations);
            
            result.ShouldFulfill(x =>
            {
                x.ExitCondition.Should().Be(Converged);
                x.IsValid.Should().BeTrue();
                x.NumberOfVariables.Should().Be(4);
                x.CostValue.Should().BeApproximately(0.1249);
                x.Parameters.Should().Equal("c0", "c1", "c2", "c3");
                x.ParameterValues.Should().BeApproximately([9.974, -1.959, 0.9898, -0.09931]);
                x.ParameterCovarianceMatrix.Should().BeApproximately(new[,]
                {
                    { 0.004391, -0.003358, 0.0006878, -4.115e-05 },
                    { -0.003358, 0.003843, -0.0009193, 5.977e-05 },
                    { 0.0006878, -0.0009193, 0.0002371, -1.614e-05 },
                    { -4.115e-05, 5.977e-05, -1.614e-05, 1.132e-06 }
                });
            });
        }
        
        [Test]
        [Description("This is a concrete example where the minimizer exits the valid parameter space by assigning a " +
                     "negative value to a bell curve's variance, causing a NaN in the cost value. The issue can be " +
                     "avoided by enforcing a lower bound of zero. If omitted, we still return the final minimization " +
                     "state with clear error details.")]
        public void leaving_the_valid_parameter_space_during_the_minimization_process_yields_an_invalid_result_with_non_finite_value_exit_condition_and_undefined_covariances_representing_the_last_state_of_the_process()
        {
            var problem = new BellCurveLeastSquaresProblem();
            var cost = problem.Cost.Build();
            var parameterConfigurations = problem.ParameterConfigurations
                .WithParameter(0).WithValue(6.41).And
                .WithParameter(1).WithValue(1.42).Build();
            var minimizerConfiguration = new MinimizerConfiguration(Strategy.Rigorous);
        
            var result = MigradMinimizer.Minimize(cost, parameterConfigurations, minimizerConfiguration);
        
            result.ShouldFulfill(x =>
            {
                x.IsValid.Should().BeFalse();
                x.ExitCondition.Should().Be(NonFiniteValue);
                x.ParameterCovarianceMatrix.Should().BeNull();
                x.CostValue.Should().Be(cost.ValueFor(x.ParameterValues)).And.NotBeFinite();
            });
        }
    }

    public class When_minimizing_a_sum_of_least_squares_cost_functions
    {
        private readonly ConfigurableLeastSquaresProblem _problem = new CubicPolynomialLeastSquaresProblem();

        [Test]
        [Description("Ensures that auto-adjustment of the error definition for least squares cost functions with " +
                     "unknown y-errors and, hence, parameter covariances works (on a per-cost basis).")]
        public void not_sharing_any_parameters_where_some_have_unknown_data_errors_and_subsequently_applying_error_refinement_yields_a_result_equivalent_to_the_results_for_the_isolated_components(
                [Values] bool hasGradient, 
                [Values] Strategy strategy)
        {
            var component1 = _problem.Cost.WithParameterSuffixes("1").WithGradient(hasGradient).Build();
            var component2 = _problem.Cost.WithParameterSuffixes("2").WithGradient(hasGradient).WithUnknownYErrors().Build();
            var sum = CostFunction.Sum(component1, component2);
            var parameterConfigurations1 = _problem.ParameterConfigurations.WithSuffix("1").Build();
            var parameterConfigurations2 = _problem.ParameterConfigurations.WithSuffix("2").Build();
            var minimizerConfiguration = new MaximumAccuracyMinimizerConfiguration(strategy);
            
            var component1Result = MinimizeAndRefineErrors(component1, parameterConfigurations1, minimizerConfiguration);
            var component2Result = MinimizeAndRefineErrors(component2, parameterConfigurations2, minimizerConfiguration);
            var sumResult = MinimizeAndRefineErrors(sum, parameterConfigurations1.Concat(parameterConfigurations2).ToArray(), minimizerConfiguration);

            sumResult.ShouldFulfill(x =>
            {
                x.CostValue.Should().BeApproximately(component1Result.CostValue + component2Result.CostValue);
                x.ParameterValues.Should().BeApproximately(component1Result.ParameterValues.Concat(component2Result.ParameterValues));
                x.ParameterCovarianceMatrix.Should()
                    .BeApproximately(component1Result.ParameterCovarianceMatrix.BlockConcat(component2Result.ParameterCovarianceMatrix), 
                        relativeTolerance: 0.004);
            });
        }

        [TestCase(false, false, 182)]
        [TestCase(true, false, 182)]
        [TestCase(true, true, 120)]
        public void with_defined_data_uncertainties_yields_the_expected_result(
            bool hasFirstGradient, 
            bool hasLastGradient, 
            int expectedFunctionCalls)
        {
            var cost = CostFunction.Sum(
                _problem.Cost.WithGradient(hasFirstGradient).Build(),
                _problem.Cost.WithGradient(hasLastGradient).WithParameterSuffixes("1", indicesToSuffix: [1, 3]).Build());
            var parameterConfigurations = _problem.ParameterConfigurations.Build()
                .Concat(_problem.ParameterConfigurations.WithSuffix("1")
                    .WithParameter(1).WithValue(-2.1).And
                    .WithParameter(3).WithValue(-0.15).Build());

            var result = MigradMinimizer.Minimize(cost, parameterConfigurations.ToArray());
            
            result.ShouldFulfill(x =>
            {
                x.ExitCondition.Should().Be(Converged);
                x.IsValid.Should().BeTrue();
                x.NumberOfVariables.Should().Be(6);
                x.NumberOfFunctionCalls.Should().BeCloseTo(expectedFunctionCalls);
                x.CostValue.Should().BeApproximately(24.99);
                x.Parameters.Should().Equal("c0", "c1", "c2", "c3", "c1_1", "c3_1");
                x.ParameterValues.Should().BeApproximately([9.974, -1.959, 0.9898, -0.09931, -1.959, -0.09931]);
                x.ParameterCovarianceMatrix.Should().BeApproximately(new[,]
                {
                    { 0.002811, -0.00215, 0.0004404, -2.635e-05, -0.00215, -2.635e-05 },
                    { -0.00215, 0.002512, -0.0005887, 3.752e-05, 0.00241, 3.902e-05 },
                    { 0.0004404, -0.0005887, 0.0001518, -1.033e-05, -0.0005887, -1.033e-05 },
                    { -2.635e-05, 3.752e-05, -1.033e-05, 7.383e-07, 3.902e-05, 7.12e-07 },
                    { -0.00215, 0.00241, -0.0005887, 3.902e-05, 0.002512, 3.752e-05 },
                    { -2.635e-05, 3.902e-05, -1.033e-05, 7.12e-07, 3.752e-05, 7.384e-07 }
                });
            });
        }
        
        [Test]
        public void with_some_having_unknown_data_errors_yields_the_expected_result(
            [Values] bool hasFirstGradient, 
            [Values] bool hasLastGradient)
        {
            var cost = CostFunction.Sum(
                _problem.Cost.WithGradient(hasFirstGradient).Build(),
                _problem.Cost.WithGradient(hasLastGradient).WithUnknownYErrors().Build());
            var parameterConfigurations = _problem.ParameterConfigurations.Build();

            var result = MigradMinimizer.Minimize(cost, parameterConfigurations);
            
            result.ShouldFulfill(x =>
            {
                x.ExitCondition.Should().Be(Converged);
                x.IsValid.Should().BeTrue();
                x.NumberOfVariables.Should().Be(4);
                x.CostValue.Should().BeApproximately(12.62);
                x.Parameters.Should().Equal("c0", "c1", "c2", "c3");
                x.ParameterValues.Should().BeApproximately([9.974, -1.959, 0.9898, -0.09931]);
                x.ParameterCovarianceMatrix.Should().BeApproximately(new[,]
                {
                    { 0.005567, -0.004258, 0.0008721, -5.218e-05 },
                    { -0.004258, 0.004873, -0.001166, 7.578e-05 },
                    { 0.0008721, -0.001166, 0.0003006, -2.046e-05 },
                    { -5.218e-05, 7.578e-05, -2.046e-05, 1.436e-06 }
                });
            });
        }

        [Test]
        [Description("Ensures parameter covariances are adjusted only for (inner) cost functions with unknown y-errors.")]
        public void with_some_having_unknown_data_errors_and_subsequently_applying_error_refinement_yields_the_expected_result(
            [Values] bool hasFirstGradient, 
            [Values] bool hasLastGradient)
        {
            var cost = CostFunction.Sum(
                _problem.Cost.WithGradient(hasFirstGradient).Build(),
                _problem.Cost.WithGradient(hasLastGradient).WithUnknownYErrors().Build());
            var parameterConfigurations = _problem.ParameterConfigurations.Build();

            var result = MinimizeAndRefineErrors(cost, parameterConfigurations);
            
            result.ShouldFulfill(x =>
            {
                x.ExitCondition.Should().Be(Converged);
                x.IsValid.Should().BeTrue();
                x.NumberOfVariables.Should().Be(4);
                x.CostValue.Should().BeApproximately(12.62);
                x.Parameters.Should().Equal("c0", "c1", "c2", "c3");
                x.ParameterValues.Should().BeApproximately([9.974, -1.959, 0.9898, -0.09931]);
                x.ParameterCovarianceMatrix.Should().BeApproximately(new[,]
                {
                    { 0.002465, -0.001886, 0.0003862, -2.311e-05 },
                    { -0.001886, 0.002158, -0.0005162, 3.356e-05 },
                    { 0.0003862, -0.0005162, 0.0001331, -9.062e-06 },
                    { -2.311e-05, 3.356e-05, -9.062e-06, 6.359e-07 }
                });
            });
        }
    }
}