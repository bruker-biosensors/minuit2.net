using AwesomeAssertions;
using AwesomeAssertions.Execution;
using minuit2.net;
using minuit2.net.CostFunctions;
using minuit2.net.Minimizers;
using minuit2.UnitTests.TestUtilities;
using static minuit2.net.MinimizationExitCondition;

namespace minuit2.UnitTests;

[TestFixture, 
 Description("Absolute expectations are generated by iminuit, the Minuit2 wrapper for Python, for the same scenarios.")]
public class The_migrad_minimizer() : Any_parameter_uncertainty_resolving_minimizer(MigradMinimizer)
{
    private static readonly IMinimizer MigradMinimizer = Minimizer.Migrad;
    
    private static IMinimizationResult MinimizeAndRefineErrors(
        ICostFunction cost,
        ParameterConfiguration[] parameterConfigurations, 
        MinimizerConfiguration? minimizerConfiguration = null)
    {
        var result = MigradMinimizer.Minimize(cost, parameterConfigurations, minimizerConfiguration);
        var adjustedCost = cost.WithErrorDefinitionAdjustedWhereRequiredBasedOn(result);
        return HesseErrorCalculator.Refine(result, adjustedCost);
    }
    
    public class When_minimizing_a_least_squares_cost_function
    {
        private readonly ConfigurableLeastSquaresProblem _problem = new CubicPolynomialLeastSquaresProblem();
        
        [TestCase(false, 100), 
         TestCase(true, 78)]
        public void yields_the_expected_result(bool hasGradient, int expectedFunctionCalls)
        {
            var cost = _problem.Cost.WithGradient(hasGradient).Build();
            var parameterConfigurations = _problem.ParameterConfigurations.Build();
            
            var result = MigradMinimizer.Minimize(cost, parameterConfigurations);

            result.Should()
                .HaveExitCondition(Converged).And
                .HaveIsValid(true).And
                .HaveNumberOfVariables(4).And
                .HaveNumberOfFunctionCallsCloseTo(expectedFunctionCalls).And
                .HaveCostValue(12.49).And
                .HaveParameters(["c0", "c1", "c2", "c3"]).And
                .HaveParameterValues([9.974, -1.959, 0.9898, -0.09931]).And
                .HaveParameterCovarianceMatrix(new[,]
                {
                    { 0.005623, -0.004301, 0.000881, -5.271e-05 },
                    { -0.004301, 0.004923, -0.001177, 7.655e-05 },
                    { 0.000881, -0.001177, 0.0003037, -2.067e-05 },
                    { -5.271e-05, 7.655e-05, -2.067e-05, 1.45e-06 }
                });
        }
        
        [TestCase(false, 31), 
         TestCase(true, 31)]
        public void with_fixed_parameters_yields_the_expected_result(bool hasGradient, int expectedFunctionCalls)
        {
            var cost = _problem.Cost.WithGradient(hasGradient).Build();
            var parameterConfigurations = _problem.ParameterConfigurations
                .WithParameterAtIndex(1).Fixed().And
                .WithParameterAtIndex(3).Fixed()
                .Build();

            var result = MigradMinimizer.Minimize(cost, parameterConfigurations);

            result.Should()
                .HaveExitCondition(Converged).And
                .HaveIsValid(true).And
                .HaveNumberOfVariables(2).And
                .HaveNumberOfFunctionCallsCloseTo(expectedFunctionCalls).And
                .HaveCostValue(437.7).And
                .HaveParameters(["c0", "c1", "c2", "c3"]).And
                .HaveParameterValues([9.411, -1.97, 1.088, -0.11]).And
                .HaveParameterCovarianceMatrix(new[,]
                {
                    { 0.001092, 0.0, -1.918e-05, 0.0 },
                    { 0.0, 0.0, 0.0, 0.0 },
                    { -1.918e-05, 0.0, 6.211e-07, 0.0 },
                    { 0.0, 0.0, 0.0, 0.0 }
                });
        }
        
        [Test]
        public void with_limited_parameters_yields_the_expected_result()
        {
            var cost = _problem.Cost.Build();
            var parameterConfigurations = _problem.ParameterConfigurations
                .WithParameterAtIndex(0).WithLimits(10.5, null).And
                .WithParameterAtIndex(3).WithLimits(null, -0.105)
                .Build();
            
            var result = MigradMinimizer.Minimize(cost, parameterConfigurations);

            result.Should()
                .HaveExitCondition(Converged).And
                .HaveIsValid(true).And
                .HaveNumberOfVariables(4).And
                .HaveNumberOfFunctionCallsCloseTo(444).And
                .HaveCostValue(62.34).And
                .HaveParameters(["c0", "c1", "c2", "c3"]).And
                .HaveParameterValues([10.5, -2.39, 1.082, -0.105]).And
                .HaveParameterCovarianceMatrix(new[,]
                {
                    { 7.023e-09, -3.654e-09, 3.124e-10, -1.261e-14 },
                    { -3.654e-09, 0.0002602, -3.344e-05, 5.468e-09 },
                    { 3.124e-10, -3.344e-05, 4.594e-06, -1.873e-09 },
                    { -1.261e-14, 5.468e-09, -1.873e-09, 1.211e-10 }
                }, relativeTolerance: 0.003);
        }
        
        [Test]
        public void with_an_analytical_gradient_and_with_limited_parameters_yields_the_expected_result()
        {
            var cost = _problem.Cost.WithGradient().Build();
            var parameterConfigurations = _problem.ParameterConfigurations
                .WithParameterAtIndex(0).WithLimits(10.5, null).And
                .WithParameterAtIndex(3).WithLimits(null, -0.105)
                .Build();

            var result = MigradMinimizer.Minimize(cost, parameterConfigurations);

            // Covariances are slightly different from the numerical-gradient case (above).
            // This is because, in contrast to proper convergence, in case of early termination at parameter limits the
            // assumption of local quadratic behavior (parabolic approximation) around the terminal cost value is
            // generally not fulfilled. Therefore, covariances are not fully robust.
            result.Should()
                .HaveExitCondition(Converged).And
                .HaveIsValid(true).And
                .HaveNumberOfVariables(4).And
                .HaveNumberOfFunctionCallsCloseTo(156).And
                .HaveCostValue(62.34).And
                .HaveParameters(["c0", "c1", "c2", "c3"]).And
                .HaveParameterValues([10.5, -2.39, 1.082, -0.105]).And
                .HaveParameterCovarianceMatrix(new[,]
                {
                    { 5.482e-09, -2.933e-09, 2.507e-10, -7.976e-15 },
                    { -2.933e-09, 0.0002602, -3.343e-05, 4.309e-09 },
                    { 2.507e-10, -3.343e-05, 4.589e-06, -1.476e-09 },
                    { -7.976e-15, 4.309e-09, -1.476e-09, 9.18e-11 }
                });
        }
        
        [Test]
        public void with_unknown_data_errors_yields_the_expected_result([Values] bool hasGradient)
        {
            var cost = _problem.Cost.WithUnknownYErrors().WithGradient(hasGradient).Build();
            var parameterConfigurations = _problem.ParameterConfigurations.Build();
            
            var result = MigradMinimizer.Minimize(cost, parameterConfigurations);

            result.Should()
                .HaveExitCondition(Converged).And
                .HaveIsValid(true).And
                .HaveNumberOfVariables(4).And
                .HaveCostValue(0.1249).And
                .HaveParameters(["c0", "c1", "c2", "c3"]).And
                .HaveParameterValues([9.974, -1.959, 0.9898, -0.09931]).And
                .HaveParameterCovarianceMatrix(new[,]
                {
                    { 0.5623, -0.4301, 0.08809, -0.00527 },
                    { -0.4301, 0.4922, -0.1177, 0.007654 },
                    { 0.08809, -0.1177, 0.03036, -0.002067 },
                    { -0.00527, 0.007654, -0.002067, 0.000145 }
                });
        }
        
        [Test]
        public void with_unknown_data_errors_and_subsequently_applying_error_refinement_yields_the_expected_result(
            [Values] bool hasGradient)
        {
            var cost = _problem.Cost.WithUnknownYErrors().WithGradient(hasGradient).Build();
            var parameterConfigurations = _problem.ParameterConfigurations.Build();
            
            var result = MinimizeAndRefineErrors(cost, parameterConfigurations);

            result.Should()
                .HaveExitCondition(Converged).And
                .HaveIsValid(true).And
                .HaveNumberOfVariables(4).And
                .HaveCostValue(0.1249).And
                .HaveParameters(["c0", "c1", "c2", "c3"]).And
                .HaveParameterValues([9.974, -1.959, 0.9898, -0.09931]).And
                .HaveParameterCovarianceMatrix(new[,]
                {
                    { 0.004391, -0.003358, 0.0006878, -4.115e-05 },
                    { -0.003358, 0.003843, -0.0009193, 5.977e-05 },
                    { 0.0006878, -0.0009193, 0.0002371, -1.614e-05 },
                    { -4.115e-05, 5.977e-05, -1.614e-05, 1.132e-06 }
                });
        }
    }

    public class When_minimizing_a_sum_of_least_squares_cost_functions
    {
        private readonly ConfigurableLeastSquaresProblem _problem = new CubicPolynomialLeastSquaresProblem();
        
        [Test, 
         Description("Ensures that auto-adjustment of the error definition for least squares cost functions with " +
                     "unknown y-errors and, hence, parameter covariances works (on a per-cost basis).")]
        public void not_sharing_any_parameters_where_some_have_unknown_data_errors_and_subsequently_applying_error_refinement_yields_a_result_equivalent_to_the_results_for_the_isolated_components(
                [Values] bool hasGradient, 
                [Values] Strategy strategy)
        {
            var component1 = _problem.Cost.WithParametersSuffixedBy("1").WithGradient(hasGradient).Build();
            var component2 = _problem.Cost.WithParametersSuffixedBy("2").WithGradient(hasGradient).WithUnknownYErrors().Build();
            var sum = CostFunction.Sum(component1, component2);
            var parameterConfigurations1 = _problem.ParameterConfigurations.WithSuffix("1").Build();
            var parameterConfigurations2 = _problem.ParameterConfigurations.WithSuffix("2").Build();
            // A minimum tolerance is used to prevent premature termination when using the fast strategy.
            var minimizerConfiguration = new MinimizerConfiguration(strategy, Tolerance: 0);
            
            var component1Result = MinimizeAndRefineErrors(component1, parameterConfigurations1, minimizerConfiguration);
            var component2Result = MinimizeAndRefineErrors(component2, parameterConfigurations2, minimizerConfiguration);
            var sumResult = MinimizeAndRefineErrors(sum, parameterConfigurations1.Concat(parameterConfigurations2).ToArray(), minimizerConfiguration);

            using (new AssertionScope())
            {
                sumResult.Should()
                    .HaveCostValue(component1Result.CostValue + component2Result.CostValue).And
                    .HaveParameterValues(component1Result.ParameterValues.Concat(component2Result.ParameterValues).ToArray());

                sumResult.ParameterCovarianceMatrix.Should()
                    .BeEquivalentTo(component1Result.ParameterCovarianceMatrix.BlockConcat(component2Result.ParameterCovarianceMatrix), 
                        options => options.WithRelativeDoubleTolerance(0.004));
            }
        }
        
        [TestCase(false, false, 182),
         TestCase(true, false, 182),
         TestCase(true, true, 120)]
        public void with_defined_data_uncertainties_yields_the_expected_result(
            bool hasFirstGradient, 
            bool hasLastGradient, 
            int expectedFunctionCalls)
        {
            var cost = CostFunction.Sum(
                _problem.Cost.WithGradient(hasFirstGradient).Build(),
                _problem.Cost.WithGradient(hasLastGradient).WithParametersSuffixedBy("1", [1, 3]).Build());
            var parameterConfigurations = _problem.ParameterConfigurations.Build()
                .CombinedWith(_problem.ParameterConfigurations
                    .WithParameterAtIndex(1).WithSuffix("1").WithValue(-2.1).And
                    .WithParameterAtIndex(3).WithSuffix("1").WithValue(-0.15).Build());

            var result = MigradMinimizer.Minimize(cost, parameterConfigurations);

            result.Should()
                .HaveExitCondition(Converged).And
                .HaveIsValid(true).And
                .HaveNumberOfVariables(6).And
                .HaveNumberOfFunctionCallsCloseTo(expectedFunctionCalls).And
                .HaveCostValue(24.99).And
                .HaveParameters(["c0", "c1", "c2", "c3", "c1_1", "c3_1"]).And
                .HaveParameterValues([9.974, -1.959, 0.9898, -0.09931, -1.959, -0.09931]).And
                .HaveParameterCovarianceMatrix(new[,]
                {
                    { 0.002811, -0.00215, 0.0004404, -2.635e-05, -0.00215, -2.635e-05 },
                    { -0.00215, 0.002512, -0.0005887, 3.752e-05, 0.00241, 3.902e-05 },
                    { 0.0004404, -0.0005887, 0.0001518, -1.033e-05, -0.0005887, -1.033e-05 },
                    { -2.635e-05, 3.752e-05, -1.033e-05, 7.383e-07, 3.902e-05, 7.12e-07 },
                    { -0.00215, 0.00241, -0.0005887, 3.902e-05, 0.002512, 3.752e-05 },
                    { -2.635e-05, 3.902e-05, -1.033e-05, 7.12e-07, 3.752e-05, 7.384e-07 }
                });
        }
        
        [Test]
        public void with_some_having_unknown_data_errors_yields_the_expected_result(
            [Values] bool hasFirstGradient, 
            [Values] bool hasLastGradient)
        {
            var cost = CostFunction.Sum(
                _problem.Cost.WithGradient(hasFirstGradient).Build(),
                _problem.Cost.WithGradient(hasLastGradient).WithUnknownYErrors().Build());
            var parameterConfigurations = _problem.ParameterConfigurations.Build();

            var result = MigradMinimizer.Minimize(cost, parameterConfigurations);

            result.Should()
                .HaveExitCondition(Converged).And
                .HaveIsValid(true).And
                .HaveNumberOfVariables(4).And
                .HaveCostValue(12.62).And
                .HaveParameters(["c0", "c1", "c2", "c3"]).And
                .HaveParameterValues([9.974, -1.959, 0.9898, -0.09931]).And
                .HaveParameterCovarianceMatrix(new[,]
                {
                    { 0.005567, -0.004258, 0.0008721, -5.218e-05 },
                    { -0.004258, 0.004873, -0.001166, 7.578e-05 },
                    { 0.0008721, -0.001166, 0.0003006, -2.046e-05 },
                    { -5.218e-05, 7.578e-05, -2.046e-05, 1.436e-06 }
                });
        }
        
        [Test, 
         Description("Ensures parameter covariances are adjusted only for the (inner) cost functions with unknown y-errors.")]
        public void with_some_having_unknown_data_errors_and_subsequently_applying_error_refinement_yields_the_expected_result(
            [Values] bool hasFirstGradient, 
            [Values] bool hasLastGradient)
        {
            var cost = CostFunction.Sum(
                _problem.Cost.WithGradient(hasFirstGradient).Build(),
                _problem.Cost.WithGradient(hasLastGradient).WithUnknownYErrors().Build());
            var parameterConfigurations = _problem.ParameterConfigurations.Build();

            var result = MinimizeAndRefineErrors(cost, parameterConfigurations);

            result.Should()
                .HaveExitCondition(Converged).And
                .HaveIsValid(true).And
                .HaveNumberOfVariables(4).And
                .HaveCostValue(12.62).And
                .HaveParameters(["c0", "c1", "c2", "c3"]).And
                .HaveParameterValues([9.974, -1.959, 0.9898, -0.09931]).And
                .HaveParameterCovarianceMatrix(new[,]
                {
                    { 0.002465, -0.001886, 0.0003862, -2.311e-05 },
                    { -0.001886, 0.002158, -0.0005162, 3.356e-05 },
                    { 0.0003862, -0.0005162, 0.0001331, -9.062e-06 },
                    { -2.311e-05, 3.356e-05, -9.062e-06, 6.359e-07 }
                });
        }
    }
}